@inproceedings{
    Hinton1995BayesianLF,
    title  = {Bayesian learning for neural networks},
    author = {Geoffrey E. Hinton and Radford M. Neal},
    year   = {1995}
}
@inproceedings{
    DLGP2018,
    title	= {Deep Neural Networks as Gaussian Processes},
    author	= {Jaehoon Lee and Yasaman Bahri and Roman Novak and Sam Schoenholz and Jeffrey Pennington and Jascha Sohl-dickstein},
    year	= {2018},
    URL	    = {https://openreview.net/pdf?id=B1EA-M-0Z}
}
@incollection{
    KMDL2009,
    title = {Kernel Methods for Deep Learning},
    author = {Youngmin Cho and Lawrence K. Saul},
    booktitle = {Advances in Neural Information Processing Systems 22},
    editor = {Y. Bengio and D. Schuurmans and J. D. Lafferty and C. K. I. Williams and A. Culotta},
    pages = {342--350},
    year = {2009},
    publisher = {Curran Associates, Inc.},
    url = {http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf}
}
@inproceedings{
    LawrenceGPLVM2004,
    author = {Lawrence, Neil D.},
    title = {Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data},
    booktitle = {Proceedings of the 16th International Conference on Neural Information Processing Systems},
    series = {NIPS'03},
    year = {2003},
    location = {Whistler, British Columbia, Canada},
    pages = {329--336},
    numpages = {8},
    url = {http://dl.acm.org/citation.cfm?id=2981345.2981387},
    acmid = {2981387},
    publisher = {MIT Press},
    address = {Cambridge, MA, USA},
} 
@article{
    LawrenceGPLVM2005,
    author = {Lawrence, Neil},
    title = {Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models},
    journal = {J. Mach. Learn. Res.},
    issue_date = {12/1/2005},
    volume = {6},
    month = dec,
    year = {2005},
    issn = {1532-4435},
    pages = {1783--1816},
    numpages = {34},
    url = {http://dl.acm.org/citation.cfm?id=1046920.1194904},
    acmid = {1194904},
    publisher = {JMLR.org},
} 
@incollection{
    SparseGP2006,
    title = {Sparse Gaussian Processes using Pseudo-inputs},
    author = {Edward Snelson and Ghahramani, Zoubin},
    booktitle = {Advances in Neural Information Processing Systems 18},
    editor = {Y. Weiss and B. Sch\"{o}lkopf and J. C. Platt},
    pages = {1257--1264},
    year = {2006},
    publisher = {MIT Press},
    url = {http://papers.nips.cc/paper/2857-sparse-gaussian-processes-using-pseudo-inputs.pdf}
}
@InProceedings{
    VariationalSparseGP2009,
    title = {Variational Learning of Inducing Variables in Sparse Gaussian Processes},
    author = {Michalis Titsias},
    booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
    pages = {567--574},
    year = {2009},
    editor = {David van Dyk and Max Welling},
    volume = {5},
    series = {Proceedings of Machine Learning Research},
    address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
    month = {16--18 Apr},
    publisher = {PMLR},
    pdf = {http://proceedings.mlr.press/v5/titsias09a/titsias09a.pdf},
    url = {http://proceedings.mlr.press/v5/titsias09a.html},
    abstract = {Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs  are defined to be variational parameters  which are selected by minimizing  the Kullback-Leibler divergence between  the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.}
}
@InProceedings{
    BayesGP2010,
    title = {Bayesian Gaussian Process Latent Variable Model},
    author = {Michalis Titsias and Neil D. Lawrence},
    booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
    pages = {844--851},
    year = {2010},
    editor = {Yee Whye Teh and Mike Titterington},
    volume = {9},
    series = {Proceedings of Machine Learning Research},
    address = {Chia Laguna Resort, Sardinia, Italy},
    month = {13--15 May},
    publisher = {PMLR},
    pdf = {http://proceedings.mlr.press/v9/titsias10a/titsias10a.pdf},
    url = {http://proceedings.mlr.press/v9/titsias10a.html},
    abstract = {We introduce a variational inference framework for training the Gaussian process latent variable model and thus performing Bayesian nonlinear dimensionality reduction. This method allows us to variationally integrate out the input variables of the Gaussian process and compute a lower bound on the exact marginal likelihood of the nonlinear latent variable model. The maximization of the variational lower bound provides a Bayesian training procedure that is robust to overfitting and can automatically select the dimensionality of the nonlinear latent space. We demonstrate our method on real world datasets. The focus in this paper is on dimensionality reduction problems, but the methodology is more general. For example, our algorithm is immediately applicable for training Gaussian process models in the presence of missing or uncertain inputs.}
}
@inproceedings{
    GPBIGDATA2013,
    author = {Hensman, James and Fusi, Nicol\`{o} and Lawrence, Neil D.},
    title = {Gaussian Processes for Big Data},
    booktitle = {Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence},
    series = {UAI'13},
    year = {2013},
    location = {Bellevue, WA},
    pages = {282--290},
    numpages = {9},
    url = {http://dl.acm.org/citation.cfm?id=3023638.3023667},
    acmid = {3023667},
    publisher = {AUAI Press},
    address = {Arlington, Virginia, United States},
} 