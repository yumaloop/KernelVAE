\documentclass[11pt,a4j]{jarticle}

% パッケージ
\usepackage[dvipdfmx]{graphicx}
\usepackage{authblk}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\newtheorem{dfn}{定義}
\newtheorem{thm}{定理}
\newtheorem{lem}{補題}
\newtheorem{exm}{例}
 
% タイトル
\title{Kernel VAE: カーネル法を用いた次元削減}
\date{2019/10/11}
\author[1]{内海 佑麻\thanks{uchiumi@ailab.ics.keio.ac.jp}}
\affil[1]{慶應義塾大学理工学部情報工学科}

\begin{document}
  % タイトル
  \maketitle
  
  % Abstract
  \begin{abstract}
    ...
  \end{abstract}

  % 目次
  \tableofcontents

  \section{Introduction}
    ...
  \section{ガウス過程と確率モデル}

    \subsection{ガウス分布}
    \paragraph{分布の再生性}
      任意の$n \in \mathbb{N}$と${\{ a_i, b_i \in \mathbb{R} \}}_{i=1}^{n}$に対して，次が成り立つ．
      \begin{align}
        &X_i \sim i.i.d. ~ \mathcal{N}(\mu_i, \sigma_i^2), ~~~ (i=1,\dots,n) \\
        &\Rightarrow \sum_{i=1}^{n} a_i X_i + b_i \sim \mathcal{N} \left( \sum_{i=1}^{n} a_i \mu_i + b_i, \sum_{i=1}^{n} a_i^2 \sigma_i^2 \right)
      \end{align}

    \paragraph{条件つき分布の計算}
      任意の2つのベクトル${\bf f}_n \in \mathbb{R}^{n}, {\bf f}_m \in \mathbb{R}^{m}$に対して，
      \begin{align}
        \left(
          \begin{array}{c}
            {\bf f}_n \\ {\bf f}_m
          \end{array}
        \right)
        \sim
        \mathcal{N} 
        \left(
          \left(
            \begin{array}{c}
              {\boldsymbol \mu}_n \\ {\boldsymbol \mu}_m
            \end{array}
          \right)
          ,
          \begin{bmatrix}
            \Sigma_{nn} & \Sigma_{nm} \\
            \Sigma_{nm}^{\mathrm{T}} & \Sigma_{mm}
          \end{bmatrix}
        \right)
      \end{align}
      ならば，次が成り立つ．
      \begin{align}
        {\bf f}_m | {\bf f}_n \sim \mathcal{N}\left( {\boldsymbol \mu}_{m|n}, \Sigma_{m|n} \right) \label{form:gaussian_conditional}  
      \end{align}
      \begin{align}          
        where ~ 
        \begin{cases}
          {\boldsymbol \mu}_{m|n} = {\boldsymbol \mu}_{m} + \Sigma_{nm}^{\mathrm{T}} \Sigma_{nn}^{-1} ( {\bf f}_n - {\boldsymbol \mu}_{n} ) \\
          \Sigma_{m|n} = \Sigma_{mm} - \Sigma_{nm}^{\mathrm{T}} \Sigma_{nn}^{-1} \Sigma_{nm}
        \end{cases} \nonumber
      \end{align}

    \subsection{ガウス過程(GP)}
      入力空間$\mathcal{X}$上の関数$f: \mathcal{X} \to \mathbb{R}$がガウス過程(Gaussian Process, GP)に従うとは，$\mathcal{X}$上の任意の$n$点${ \{ x_i \} }_{i=1}^{n}$に対して，
      ベクトル${\bf f} = { ( f(x_1),\dots,f(x_n) ) }^{\mathrm{T}} \in \mathbb{R}^{n}$が$n$次元ガウス分布に従うことをいう．
      ここで，確率変数${\bf f} \in \mathbb{R}^{n}$が$n$次元ガウス分布に従う時，その確率密度関数$p({\bf f})$は，平均関数${m}(\cdot)$と共分散関数$v(\cdot,\cdot)$を用いて
      \begin{align}
        p({\bf f}) = \frac{1}{ {(2 \pi)}^{n/2} {| V({\bf f}) |}^{1/2} } \exp \left( {- \frac{1}{2}} {({\bf f} - m(\bf f))}^{\mathrm{T}} {V({\bf f})}^{-1} ({\bf f} - m(\bf f)) \right)
      \end{align}
      と定められる．ただし，$V({\bf f})$は共分散$v({\bf f}_i, {\bf f}_j)$を$ij$要素にもつ共分散行列である．
      ゆえに，関数$f: \mathcal{X} \to \mathbb{R}$がガウス過程(Gaussian Process, GP)に従うとき，その挙動は平均関数${m}(\cdot)$と共分散関数$v(\cdot,\cdot)$によって定められ，これを以下のように記述する．
      \begin{align}
        f(\cdot) \sim \mathcal{GP}(m(\cdot), v(\cdot,\cdot))
      \end{align}    

    \subsection{ガウス過程回帰(GPR)}
      確率変数$X \in \mathbb{R}^{d}, Y \in \mathbb{R}$の実現値からなる$n$個のデータサンプル$\mathcal{D} = {\{ {\bf x}_i, y_i \}}_{i=1}^n$
      を用いて，$X$の値から$Y$の値を推定するモデル$f:X \to Y$を特定することを回帰問題という．
      すべての$({\bf x}, y)$に対して，モデルの出力値$f({\bf x})$と$y$との誤差を$\varepsilon$とおき，これが正規分布$\mathcal{N}(0, \sigma^2)$に従うと仮定すると回帰モデルは．
      \begin{align}
        y = f({\bf x}) + \varepsilon, ~~~ \varepsilon \sim \mathcal{N}(0, \sigma^2)
      \end{align}
      あるいは，正規分布の再生性より，
      \begin{align}
        y | {\bf x} \sim \mathcal{N}(f({\bf x}), \sigma^2) \label{form:reg_likelihood}
      \end{align}
      となる．
      また一般の回帰問題において，データサンプル$\mathcal{D} = {\{ X_i, Y_i \}}_{i=1}^n$とモデル$f:X \to Y$に対して下式が成り立つことから，これはモデル$f$の分布に関するベイズ推論へ拡張できる．
      \begin{align}
        p(f|\mathcal{D}) = \frac{p(\mathcal{D}|f)p(f)}{p(\mathcal{D})}, ~~i.e.~~~
        p(f | Y,X) = \frac{ p( Y | X, f) p(f) }{p( Y | X )} \label{form:reg_bayes}
      \end{align}
      上のモデルにおいて，関数$f$がガウス過程に従う場合，これをガウス過程回帰という．
      たとえば，関数$f$に対して，
      \begin{align}
        f(\cdot) \sim \mathcal{GP}(m(\cdot), k(\cdot,\cdot)) \label{form:gp_prior}
      \end{align}
      を仮定すると，${\bf f}_n = { ( f({\bf x}_1),\dots,f({\bf x}_n) ) }^{\mathrm{T}}$と${\bf y}_n = {( y_1,\dots,y_n )}^{\mathrm{T}}$に対して，
      \begin{align}
        {\bf f}_n &\sim \mathcal{N}(m(X_n), K(X_n,X_n)) \\
        {\bf y}_n &\sim \mathcal{N}(m(X_n), K(X_n,X_n) + \sigma^2 I_n)
      \end{align}
      が成り立つ．ただし，$ m(X_n) = { ( m({\bf x}_1), \dots, m({\bf x}_n) ) }^{\mathrm{T}} $，
      $K(X_n,K(X_n))_{ij} = k(f({\bf x}_i), f({\bf x}_j))$，$I_n$は$n \times n$の単位行列とする．
      式(\ref{form:gp_prior})は，式(\ref{form:reg_bayes})でモデル$f$の事前分布$p(f)$を定めることに対応する．
      さらに，式(\ref{form:reg_likelihood})と正規分布の共役性より，ガウス過程回帰では，事前分布$p(f)$と事後分布$p(f|Y,X)$が共に正規分布に従うため，
      データサンプル$\mathcal{D} = {\{ X_i, Y_i \}}_{i=1}^n$に基づく平均関数$m(\cdot)$と共分散関数$k(\cdot,\cdot)$の行列計算($O(n^2)$)のみで事後分布の形状が求められる．
      
      さらに，未知の$m$個のデータ${\{ {\bf x}_i \} }_{i=n+1}^{n+m}$に対して，対応する${\{ y_i \} }_{i=n+1}^{n+m}$の同時分布(予測分布)を求めることもできる．
      式(\ref{form:gp_prior})より，
      \begin{align}
        \left(
          \begin{array}{c}
            {\bf f}_n \\ {\bf f}_m
          \end{array}
        \right)
        \sim
        \mathcal{N} 
        \left(
          \left(
            \begin{array}{c}
              m(X_n) \\ m(X_m)
            \end{array}
          \right)
          ,
          \begin{bmatrix}
            K(X_n,X_n) & K(X_n,X_m) \\
            {K(X_n,X_m)}^{\mathrm{T}} & K(X_m,X_m)
          \end{bmatrix}
        \right)
      \end{align}
      が成り立つから，式(\ref{form:gaussian_conditional})より，${\bf f}_m$の${\bf f}_n$に対する予測分布は，
      \begin{align}
        {\bf f}_m | {\bf f}_n \sim \mathcal{N}\left( E[{\bf f}_m | {\bf f}_n], V[{\bf f}_m | {\bf f}_n] \right) 
      \end{align}
      \begin{align}          
        where ~ 
        \begin{cases}
          E[{\bf f}_m | {\bf f}_n] = m(X_m) + {K(X_n,X_m)}^{\mathrm{T}} {K(X_n,X_n)}^{-1} ( {\bf f}_n - m(X_n) ) \\
          V[{\bf f}_m | {\bf f}_n] = K(X_m,X_m) - {K(X_n,X_m)}^{\mathrm{T}} {K(X_n,X_n)}^{-1} K(X_n,X_m)
        \end{cases} \nonumber
      \end{align}
      となり，${\bf f}_m$の${\bf y}_n$に対する予測分布は，
      \begin{align}
        {\bf f}_m | {\bf y}_n \sim \mathcal{N}\left( E[{\bf f}_m | {\bf y}_n], V[{\bf f}_m | {\bf y}_n] \right) 
      \end{align}
      \begin{align}          
        where ~ 
        \begin{cases}
          E[{\bf f}_m | {\bf y}_n] = m(X_m) + {K(X_n,X_m)}^{\mathrm{T}} {( K(X_n,X_n) + \sigma^2 I_n )} ^{-1} ( {\bf y}_n - m(X_n) ) \\
          V[{\bf f}_m | {\bf y}_n] = K(X_m,X_m) - {K(X_n,X_m)}^{\mathrm{T}} { ( K(X_n,X_n) + \sigma^2 I_n ) }^{-1} K(X_n,X_m)
        \end{cases} \nonumber
      \end{align}
      となる．よって，${\bf y}_m$の${\bf y}_n$に対する予測分布は，
      \begin{align}
        {\bf y}_m | {\bf y}_n \sim \mathcal{N}\left( E[{\bf y}_m | {\bf y}_n], V[{\bf y}_m | {\bf y}_n] \right) 
      \end{align}
      \begin{align}          
        where ~ 
        \begin{cases}
          E[{\bf y}_m | {\bf y}_n] = m(X_m) + {K(X_n,X_m)}^{\mathrm{T}} {( K(X_n,X_n) + \sigma^2 I_n )} ^{-1} ( {\bf y}_n - m(X_n) ) \\
          V[{\bf y}_m | {\bf y}_n] = K(X_m,X_m) - {K(X_n,X_m)}^{\mathrm{T}} { ( K(X_n,X_n) + \sigma^2 I_n ) }^{-1} K(X_n,X_m) + \sigma^2 I_m
        \end{cases} \nonumber
      \end{align}
      となる．
    
      \subsection{ガウス過程潜在変数モデル(GP-LVM)}
        確率変数$X \in \mathbb{R}^{Q}, Y \in \mathbb{R}^{D}$に対して，$Y$のN個のデータサンプル${ \{ {\bf y}_i \} }_{i=1}^{n}$から，生成モデル$p(Y|X)$を特定することを考える．
        このとき，$Y$を観測変数，$X$を潜在変数と呼ぶ．
        観測変数の観測値(計画行列)を${\bf Y}^n = \mathbb{R}^{n \times D}$，それを表現する潜在変数の観測値(計画行列)を${\bf X}^n \in \mathbb{R}^{n \times Q}$とおく．
        すべての$i \in \{1,\dots,N\}$と$d \in \{1,\dots,D\}$に対して，関数$f: \mathbb{R}^Q \to \mathbb{R}$を用いて，生成過程：
        \begin{align}
          {{\bf y}_i}_d = f({\bf x}_i) + \varepsilon_i, ~~~
          \varepsilon_i \sim i.i.d. ~ \mathcal{N}(0, \sigma_{\varepsilon}^2)
        \end{align}
        を仮定し，さらに関数$f$に対してガウス過程
        \begin{align}
          f(\cdot) \sim \mathcal{GP}(0, k(\cdot,\cdot))
        \end{align}
        を仮定すると，
        \begin{align}
          p({\bf y}_i | {\bf X}^n) &= \prod_{d=1}^{D} p({{\bf y}_i}_d) = \prod_{d=1}^{D} \mathcal{N}({{\bf y}_i}_d | f({\bf x}_i), \sigma_\varepsilon^2 ) \\
          p({\bf y}_d | {\bf X}^n) &= \prod_{i=1}^{n} p({{\bf y}_i}_d) = \mathcal{N}({\bf y}_d | {\bf 0}, {\bf \Sigma}_{nn})
        \end{align}
        となるから，生成モデルの尤度は，次式で与えられる．
        \begin{align}
          p({\bf Y}^n|{\bf X}^n) &= \prod_{d=1}^{D} p({\bf y}_d | {\bf X}^n) = \prod_{d=1}^{D} \mathcal{N}({\bf y}_d | {\bf 0}, {\bf \Sigma}_{nn}) \\
          &= \prod_{d=1}^{D} \frac{1}{{(2 \pi)}^{\frac{n}{2}} {|{\bf \Sigma}_{nn}|}^{\frac{1}{2}}} \exp \left( - \frac{1}{2} {\bf y}_d^{\mathrm{T}} {\bf \Sigma}_{nn}^{-1} {\bf y}_d \right) \\
          &= \frac{1}{{(2 \pi)}^{\frac{Dn}{2}} {|{\bf \Sigma}_{nn}|}^{\frac{D}{2}}} \exp \left( - \frac{1}{2} {\rm tr}({\bf \Sigma}_{nn}^{-1} {{\bf Y}^n}{{\bf Y}^n}^{\mathrm{T}}) \right)
        \end{align}
        ただし，
        \begin{align}
          {\bf \Sigma}_{nn} &= {\bf K}_{nn} + \sigma_{\varepsilon}^2 {\bf I}_n \\
          {\bf K}_{nn}(i,j) &= k({\bf x}_i, {\bf x}_j), ~~~ \forall (i,j)
        \end{align}
        とする．
        \paragraph{最尤推定}
          ${\bf Y}^n$が与えられたときの${\bf X}^n$の対数尤度は，
          \begin{align}
            \log p({\bf Y}^n|{\bf X}^n)
            &= -\frac{Dn}{2} \log (2 \pi) - \frac{D}{2} \log |{\bf \Sigma}_{nn}| - \frac{1}{2} {\rm tr} ({\bf \Sigma}_{nn}^{-1} {{\bf Y}^n}{{\bf Y}^n}^{\mathrm{T}})
          \end{align}
          となるから，これを目的関数として最大化すれば，${\bf X}^n$の最尤推定量は，次のように求められる．
          \begin{align}
            \hat{\bf X}^{n}_{ML} &= \underset{{\bf X}^n}{\rm argmax} ~ \log p( {\bf Y}^n | {\bf X}^n) \\
          \end{align}

        \paragraph{MAP推定}
          $X^n$の事前分布として，
          \begin{align}
            p(X^n) 
            &= \prod_{q=1}^{Q} p({\bf x}_q) = \prod_{q=1}^{Q} \mathcal{N}({\bf x}_q | {\bf 0}, \sigma_x^2 I_N) \\
            &= \prod_{q=1}^{Q} \frac{1}{{(2 \pi {\sigma_x^2} )}^{\frac{n}{2}}} \exp \left( - \frac{ {\bf x}_q^{\mathrm{T}} {\bf x}_q }{2 \sigma_x^2} \right) \\
            &= \frac{1}{{(2 \pi {\sigma_x^2} )}^{\frac{QN}{2}}} \exp \left( -\frac{1}{2 \sigma_x^2} {\rm tr} ({{\bf X}^n}{{\bf X}^n}^{\mathrm{T}}) \right) \\
          \end{align}
          を仮定すると，ベイズの定理より，$X$の事後分布は，
          \begin{align}
            p(X|Y) = \frac{1}{Z} p(Y|X)p(X)
          \end{align}
          となる．ただし，$Z$は規格化定数とする，よって，${\bf X}^n$の事後確率は，
          \begin{align}
            p({\bf X}^n|{\bf Y}^n) 
            &\propto \log p({\bf Y}^n | {\bf X}^n) p({\bf X}^n) \\
            &= -\frac{Dn}{2} \log (2 \pi) - \frac{D}{2} \log |{\bf \Sigma}_{nn}| - \frac{1}{2} {\rm tr} ({\bf \Sigma}_{nn}^{-1} {{\bf Y}^n}{{\bf Y}^n}^{\mathrm{T}}) \\
            &\hspace{1em} -\frac{Qn}{2} \log (2 \pi \sigma_x^2) - \frac{1}{2} {\rm tr} ({{\bf X}^n}{{\bf X}^n}^{\mathrm{T}})
          \end{align}
          となるから，これを目的関数として最大化すれば，${\bf X}^n$のMAP推定量は，次のように求められる．
          \begin{align}
            \hat{\bf X}^{n}_{MAP} &= \underset{X^n}{\rm argmax} ~ \log p({\bf Y}^n | {\bf X}^n) p({\bf X}^n) 
          \end{align}

  \section{Deep Neural Networkのカーネル法による近似}
    \subsection{1層のFeed Forward Neural Network}
      ニューラルネットワークの全結合層や畳み込み層における推論計算は，線形写像と非線形の活性化関数の組み合わせによって構成される．
      ここで，ある層の入力ベクトルを${\bf x} \in \mathbb{R}^{N}$，
      出力ベクトルを${\bf y} \in \mathbb{R}^{M}$，
      線形写像に対応する変換行列を${\bf W} \in \mathbb{R}^{M \times N}$，
      活性化関数を$\phi: \mathbb{R}^{M} \to \mathbb{R}^{M}$とすると，
      非線形関数$f:{\bf x} \mapsto {\bf y}$は以下のように構成される．
      \begin{align}
        {\bf y} = f({\bf x}) = \phi( {\bf W x} )
      \end{align}
      なお，全結合層に置けるバイアスベクトルの追加に関しては，$\bf x$と$\bf W$に次元を1つ追加することで上式と等価となり，
      畳み込み層に置けるフィルタ演算もim2colによって上式と等価となることに注意する．
      ここで，関数$f$に対する内積${f(\cdot)}^{\mathrm{T}}f(\cdot): {\bf x} \times {\bf x}' \mapsto \mathbb{R}$を考える．
      活性化関数$\phi$としてReLUを仮定して，対応する指示関数
      \begin{align}
        {\mathrm{I}}(x) = 
        \begin{cases}
          1 ~ (x \geq 0) \\
          0 ~ (x < 0)
        \end{cases}
      \end{align}
      を定義すると，$\forall {\bf x},{\bf x}'$に対する内積は，
      \begin{align}
        {f({\bf x})}^{\mathrm{T}} f({\bf x}') = \sum_{i=1}^{M} 
        {\mathrm{I}}( {\bf w}_{i}^{\mathrm{T}} {\bf x} ) {\mathrm{I}}( {\bf w}_{i}^{\mathrm{T}} {\bf x}' ) 
        ( {\bf w}_{i}^{\mathrm{T}} {\bf x} ) ( {\bf w}_{i}^{\mathrm{T}} {\bf x}' )
      \end{align}
      となる．ただし，${\bf w}_i = {( {\bf W}_{i1},\dots,{\bf W}_{iN} )}^{\mathrm{T}} \in \mathbb{R}^{N}$とする．
      いま，行列$\bf W$の各要素${\bf W}_{ij}$をi.i.d.となる確率変数と仮定すると，
      すべての$i$に対して${\bf w}_i$もi.i.d.となる，また，確率変数${\bf W}_{ij}$は平均と分散として，それぞれ$0$，$\sigma_w^2$を与える．
      \begin{align}
        \forall i,j, ~ {\bf W}_{ij} \sim i.i.d. \\
        E[{\bf W}_{ij}] = 0, ~ V[{\bf W}_{ij}] = \sigma_w^2
      \end{align}
      すると，中心極限定理(Central Limit Theorem)より，${\bf x}$を固定したとき，任意の${\bf w}$の関数$C({\bf w})$に対して，
      \begin{align}
        E\left[ \frac{1}{M} \sum_{i=1}^{M} C({\bf w}_i) \right] \to
        \int d{\bf w} \frac{\exp (\frac{- {|| {\bf w} ||}^2}{2})}{{(2 \pi \sigma_w^2 )}^{N / 2}}
        C({\bf w}_i) ~~~ (M \to \infty)
      \end{align}
      が満たされるから，
      \begin{align}
        &E\left[ \frac{1}{M} {f({\bf x})}^{\mathrm{T}} f({\bf x}') \right] 
        = E\left[ \frac{1}{M} \sum_{i=1}^{M} {\mathrm{I}}( {\bf w}_{i}^{\mathrm{T}} {\bf x} ) {\mathrm{I}}( {\bf w}_{i}^{\mathrm{T}} {\bf x}' ) 
        ( {\bf w}_{i}^{\mathrm{T}} {\bf x} ) ( {\bf w}_{i}^{\mathrm{T}} {\bf x}' ) \right] 
        \to \nonumber \\
        &\int d{\bf w} \frac{\exp (\frac{- {|| {\bf w} ||}^2}{2})}{{(2 \pi \sigma_w^2 )}^{N / 2}}
        {\mathrm{I}}( {\bf w}^{\mathrm{T}} {\bf x} ) {\mathrm{I}}( {\bf w}^{\mathrm{T}} {\bf x}' ) 
        ( {\bf w}^{\mathrm{T}} {\bf x} ) ( {\bf w}^{\mathrm{T}} {\bf x}' ) ~~~ (M \to \infty) \label{form:kernel_f} 
      \end{align}
      が成り立つ．式(\ref{form:kernel_f})右辺を整理すると，以下の定理が導かれる．

      \begin{dfn}
        (arc-cosine kernel)
        任意の2つのベクトル${\bf x}, {\bf x}' \in \mathcal{X} \subset \mathbb{R}^{N}$に対して，
        \begin{align}
          \theta &= {\cos}^{-1} \left( \frac{{\bf x}^{\mathrm{T}} {\bf x}'}{ \|{\bf x}\| \|{\bf x}'\| } \right) \\
          J(\theta) &= \sin \theta + (\pi - \theta) \cos \theta
        \end{align}
        とおき，arc-cosineカーネル
        \begin{align}
          k^{(1)}({\bf x}, {\bf x}') = \frac{1}{\pi} \| {\bf x} \| \| {\bf x}' \| J(\theta) ~~ (M \to \infty)
        \end{align}
        を定義する．
      \end{dfn}

      \begin{thm}\label{thm:clt_dnn}
        各要素$W_{ij}$が互いに独立に正規分布$\mathcal{N}(0, 1)$に従うランダム行列${\bf W} \in \mathbb{R}^{M \times N}$を考える．
        活性化関数としてReLU関数$\phi$を用いて，1層のニューラルネットワークに相当する関数$f:\mathbb{R}^{N} \to \mathbb{R}^{M}$:
        \begin{align}
          f({\bf x}) = \phi({\bf Wx}) ~~ (\forall {\bf x} \in \mathbb{R}^{N})
        \end{align}
        を考える．このとき任意の${\bf x}, {\bf x}' \in \mathbb{R}^{N}$に対して，
        \begin{align}
          E \left[ \frac{1}{M} {f({\bf x})}^{\mathrm{T}} f({\bf x}') \right] \to
          \frac{1}{\sigma_w^N} k^{(1)}({\bf x}, {\bf x}') ~~ (M \to \infty)
        \end{align}
        が成り立つ．
      \end{thm}

      \begin{thm}\label{thm:semi_positive}
        $\mathcal{X} \times \mathcal{X}$上の関数
        \begin{align}
          k^{(1)}({\bf x},{\bf x}') = \frac{1}{\pi} \| {\bf x} \| \| {\bf x}' \| J(\theta) \label{form:dnn_kernel}
        \end{align}
        は半正定値関数である．
      \end{thm}

      \begin{thm}\label{thm:kernel_matrix_positive}
        (カーネル関数存在定理)
        任意の対称行列$K \in \mathbb{R}^{n \times n} $が半正定値ならば，
        データ空間$\mathcal{X}$上の$n$点${ \{ {\bf x}_i \} }_{i=1}^{n}$と，
        特徴空間$\mathcal{F} \subset \mathbb{R}^{n}$上の$n$次元特徴ベクトル${ \{ f({\bf x}_i) \} }_{i=1}^{n}$がそれぞれ存在して，
        \begin{align}
          K_{ij} = {\langle f({\bf x}_i), f({\bf x}_j) \rangle}_{\mathcal{F}} = \sum_{k=1}^{n} {f({\bf x}_i)}_k {f({\bf x}_j)}_k 
        \end{align}
        が成り立つ．さらに，$k({\bf x}_i, {\bf x}_j) = K_{ij}$とおけば，半正定値関数$k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$が定義される．
      \end{thm}

      定理\ref{thm:kernel_matrix_positive}から，カーネル関数$k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$を定義せずとも，
      与えられたデータサンプルから構成された半正定値対称行列を作ることで，それが何らかのカーネル関数によるグラム行列に相当することが保証される．
      よって，定理\ref{thm:clt_dnn}と定理\ref{thm:semi_positive}より，式(\ref{form:dnn_kernel})を共分散関数にもつガウス過程が
      1層のNeuralNetworkの近似となることがわかる．

    \subsection{多層のFeed Forward Neural Network}
      1層のFeed Forward Neural Networkに関する議論は，正定値カーネルの線形性より，容易に多層へと拡張することができる．
      一般性を失うことなく，$L > 0$層からなるFeed Forward Neural Networkを考え，各層の線形写像に対応する重み行列${\bf W}^{(l)}$に対して，固定された$\sigma^2_w$を与えて，
      \begin{align}
        \forall l \in [1, L] \subset \mathbb{N}, ~~~ {\bf W}^{(l)}_{ij} \sim i.i.d. ~ \mathcal{N}(0, \sigma^2_w)
      \end{align}
      を仮定する．
      第$l$層のユニット数を$N^{(l)}$，第$l$層の空間を$\mathcal{T}^{(l)}$，第$l$層の出力を得る関数を$f^{(l)}: \mathcal{X} \to \mathcal{T}^{(l)}$とおく．
      任意の入力データ${\bf x}, {\bf x}' \in \mathcal{X}$に対して，Neural Networkの第$l$層に対応する共分散関数$k^{(l)}: \mathcal{T}^{(l)} \times \mathcal{T}^{(l)} \to \mathbb{R}$は，次の漸化式で求められる．
      \begin{align}
        k^{(1)}({\bf x}, {\bf x}') &= Cov \left[ f^{(1)}({\bf x}), f^{(1)}({\bf x}') \right] \nonumber \\
                                   &= Cov \left[ \phi({\bf W}^{(1)} {\bf x}), \phi({\bf W}^{(1)} {\bf x}') \right] \nonumber \\ 
                                   &= \frac{1}{\pi \sigma_w^{N^{(1)}}} \| {\bf x} \| \| {\bf x}' \| J(\theta^{(0)}) \\
        k^{(l+1)}({\bf x}, {\bf x}') &= Cov \left[ f^{(l+1)}({\bf x}), f^{(l+1)}({\bf x}') \right] \nonumber \\
                                     &= Cov \left[ \phi( {\bf W}^{(l+1)} \cdots \phi( {\bf W}^{(1)} {\bf x} )), \phi( {\bf W}^{(l+1)} \cdots \phi( {\bf W}^{(1)} {\bf x}' )) \right] \nonumber \\ 
                                     &= \frac{1}{ \pi \sigma_{w}^{ N^{(l+1)}}} \sqrt{ k^{(l)}({\bf x}, {\bf x}) } \sqrt{ k^{(l)}({\bf x}', {\bf x}') } J(\theta^{(l)})
      \end{align}
      ただし，
      \begin{align}
        J(\theta^{(l)}) &= \sin \theta^{(l)} + (\pi - \theta^{(l)}) \cos \theta^{(l)} \\
        \theta^{(l)} &=
        \begin{cases}
          \cos^{-1} \left( \frac{{\bf x}^{\mathrm{T}} {\bf x}'}{ \|{\bf x}\| \|{\bf x}'\| } \right) ~~~ (l = 0) \\
          \cos^{-1} \left( \frac{k^{(l)}( {\bf x}, {\bf x}' )}{ \sqrt{ k^{(l)}({\bf x}, {\bf x}) } \sqrt{ k^{(l)}({\bf x}', {\bf x}') } } \right) ~~~ (l \neq 0) 
        \end{cases}
      \end{align}
      とする．以上の結果から，入力から第$l$層の各ユニットの出力値を得る関数$f^{(l)}_i: \mathcal{X} \to \mathbb{R}$は以下のガウス過程で近似できる．
      \begin{align}
        f^{(l)}_i(\cdot) \sim \mathcal{GP}( 0, k^{(l)}(\cdot,\cdot) ), ~~~ (i=1,\dots,N^{(l)})
      \end{align}

  \section{Kernel Variational AutoEncoder}
    前章での議論から，GP-LVMにおける潜在変数$X$と観測関数$Y$に対する確率モデル$p(Y|X)$を，
    多層のFeed Forward Neural Networkに相当する関数
    \begin{align}
      f({\bf x}) = \phi( {\bf w}^{\mathrm{T}} {\bf x})
    \end{align}
    によって表現することを考える．


  \section{Experiments}
  \section{Conclusion}

  % \bibliography{ref} %hoge.bibから拡張子を外した名前
  % \bibliographystyle{junsrt} %参考文献出力スタイル

\end{document}

